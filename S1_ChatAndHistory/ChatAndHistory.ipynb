{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d6e9013",
   "metadata": {},
   "source": [
    "### Setup Langchain + LLM\n",
    "1. Install Langchain: \n",
    "- pip intall langchain\n",
    "2. Install integration packages.\n",
    "- pip install -U langchain-cohere\n",
    "- pip install -U langchain-groq\n",
    "- pip install -U langchain-mistralai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ae4cdab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Hi there! I can help you with that. \\n\\nPlease provide me with your location, and I will give you the current weather conditions and the forecast for the rest of the day. \\n\\n- Have a great day!' additional_kwargs={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': 'a85a9ba0-8696-4867-af91-a342cff9ef6a', 'token_count': {'input_tokens': 237.0, 'output_tokens': 46.0}} response_metadata={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': 'a85a9ba0-8696-4867-af91-a342cff9ef6a', 'token_count': {'input_tokens': 237.0, 'output_tokens': 46.0}} id='run-02e3bf44-e419-49de-a307-335faca3fef7-0' usage_metadata={'input_tokens': 237, 'output_tokens': 46, 'total_tokens': 283}\n",
      "content=\"I'd be happy to help you with that!\\n\\nAccording to our latest forecast, today's weather is looking mostly sunny with a high of 72°F (22°C) and a low of 50°F (10°C). There's a gentle breeze blowing at about 5 mph (8 km/h), making it a great day to get outside and enjoy the fresh air!\\n\\nHowever, please note that there's a slight chance of scattered thunderstorms rolling in later this afternoon, so be sure to pack your umbrella just in case. But overall, it's shaping up to be a beautiful day!\\n\\nHave a great day!\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 125, 'prompt_tokens': 51, 'total_tokens': 176, 'completion_time': 0.104166667, 'prompt_time': 0.053817146, 'queue_time': 0.0001614989999999955, 'total_time': 0.157983813}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_6a6771ae9c', 'finish_reason': 'stop', 'logprobs': None} id='run-25d4abb8-6dfa-4b18-b9e4-c9ad8cb09a04-0' usage_metadata={'input_tokens': 51, 'output_tokens': 125, 'total_tokens': 176}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import configparser\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_cohere import ChatCohere\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('../config.ini')\n",
    "groq = config['groq']\n",
    "cohere = config['cohere']\n",
    "\n",
    "os.environ['GROQ_API_KEY'] = groq.get('GROQ_API_KEY')\n",
    "os.environ['COHERE_API_KEY'] = cohere.get('COHERE_API_KEY')\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content='You are a weather service. You will respond to weather queries to the best of you ability. You will always end with - Have a great day'),\n",
    "    HumanMessage(content='Hey whats the weather like today?')\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "## code for cohere.\n",
    "model = ChatCohere(model=\"command-r-plus\")\n",
    "print(model.invoke(messages))\n",
    "\n",
    "## Code for Groq\n",
    "model = ChatGroq(model=\"llama3-8b-8192\")\n",
    "print(model.invoke(messages))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "726b7ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Hello there! Thank you for reaching out to us. \\n\\nTo check the status of your order, please provide me with your order number or the email address associated with your account. Once I have that information, I can look into this for you and provide an update on your order's progress. \\n\\nIs there any additional information you can provide at this time? \\n\\nIs your query resolved? If yes, then have a great day ahead.!\" additional_kwargs={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': 'ae335f94-b70f-41b3-8f70-3afb373d3133', 'token_count': {'input_tokens': 259.0, 'output_tokens': 92.0}} response_metadata={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': 'ae335f94-b70f-41b3-8f70-3afb373d3133', 'token_count': {'input_tokens': 259.0, 'output_tokens': 92.0}} id='run-5d5b9c74-0bcb-4def-af4f-3ed935965094-0' usage_metadata={'input_tokens': 259, 'output_tokens': 92, 'total_tokens': 351}\n",
      "content=\"I'd be happy to help you with that! Can you please provide me with your order number or your email address associated with the order? That way, I can look into the status of your order and provide you with the most accurate information.\\n\\nAlso, just to confirm, have you received a confirmation email from us regarding your order placement?\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 69, 'prompt_tokens': 72, 'total_tokens': 141, 'completion_time': 0.060745115, 'prompt_time': 0.003028553, 'queue_time': 0.010090906, 'total_time': 0.063773668}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_179b0f92c9', 'finish_reason': 'stop', 'logprobs': None} id='run-d490f4c2-01f8-43a3-a67c-71b79d834cf0-0' usage_metadata={'input_tokens': 72, 'output_tokens': 69, 'total_tokens': 141}\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content='you are a ecommerce website customer support. you will respond to customer order queries to the best of your abilities. you will always end with- is your query resolved? if yes, then end your conversation with have a great day ahead.! '),\n",
    "    HumanMessage(content='hey what is the status of my order?')\n",
    "]\n",
    "\n",
    "model = ChatCohere(model=\"command-r-plus\")\n",
    "print(model.invoke(messages)) \n",
    "\n",
    "model = ChatGroq(model='llama3-8b-8192')\n",
    "print(model.invoke(messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76fa0d3-c23a-455f-998f-bb68018d0db0",
   "metadata": {},
   "source": [
    "## Create the prompt\n",
    "1. Imports Human and System message classes. System represents our instructions to GPT and Human represents the question or prompt that the user provides.\n",
    "2. LangChain responses are instances of class `BaseMessage` It contains the actual response from GPT and some other metadata.\n",
    "3. Since we are interested only in the string reponse that GPT gave we chain (pipe) the reponse to a parser\n",
    "4. For our purpose we will use `StrOutputParser` class\n",
    "5. Next we create a `chain` using the components `model` and `parser`\n",
    "6. Finally we call the `invoke` method on the chain and pass our `messages` list to it.\n",
    "7. In the output cell we get the response from `GPT-35-turbo`\n",
    "\n",
    "*A chain is an wrapper around multiple individual components that are executed in a defined order. Components in LangChain implement `Runnable` interface. This interface have a method `invoke` that transforms single input to an output.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36f9fb1d-d604-41ab-8e62-5ea4e6a9213b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<cohere.client.Client object at 0x76c7b547a2a0> async_client=<cohere.client.AsyncClient object at 0x76c7b5479d00> model='command-r' cohere_api_key=SecretStr('**********')\n",
      "['/home/codespace/.python/current/lib/python312.zip', '/home/codespace/.python/current/lib/python3.12', '/home/codespace/.python/current/lib/python3.12/lib-dynload', '', '/home/codespace/.local/lib/python3.12/site-packages', '/home/codespace/.python/current/lib/python3.12/site-packages']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"A classic lateral thinking puzzle!\\n\\nThe correct answer is: The third room with the tigers that haven't eaten for six months.\\n\\nThe reasoning is that a tiger that hasn't eaten for six months is likely to be dead or too weak to attack, making it the safest option for the condemned man.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The classes used for setting up the prompt\n",
    "import puzzles\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate #import the Class for creating a prompt\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "puzzle = puzzles.puzzles('hungryLions') # Based on user input pick a puzzle.\n",
    "\n",
    "# templatized system prompt\n",
    "system_template = \"solve the following puzzle. Please provide a {responseType} response.\" \n",
    "\n",
    "# Create prompt template instance.\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_template),\n",
    "        (\"user\", puzzle)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# prompt Template also implements runnable and can be easily chained.\n",
    "model = ChatGroq(model=\"llama3-8b-8192\")\n",
    "chain = prompt_template | model | parser\n",
    "\n",
    "chain.invoke({\"responseType\":\"brief\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c65f8fd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A classic lateral thinking puzzle!\\n\\nThe answer is: She saw a ghost.\\n\\nThe door was opened, she screamed, and then died of fright from seeing a supernatural entity.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "puzzle = puzzles.puzzles('screamDead') # Based on user input pick a puzzle.\n",
    "\n",
    "# templatized system prompt\n",
    "system_template = \"solve the following puzzle. Please provide a {responseType} response.\" \n",
    "\n",
    "# Create prompt template instance.\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_template),\n",
    "        (\"user\", puzzle)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# prompt Template also implements runnable and can be easily chained.\n",
    "model = ChatGroq(model=\"llama3-8b-8192\")\n",
    "chain = prompt_template | model | parser\n",
    "chain.invoke({\"responseType\":\"brief\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39789ed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Carbon.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ChatGroq(model=\"llama3-8b-8192\")\n",
    "chain = prompt_template | model | parser\n",
    "chain.invoke({\"responseType\":\"one word\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9498ce4c",
   "metadata": {},
   "source": [
    "### Chatbot \n",
    "1. We begin with creating a basic chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a61142d-54d4-46b8-a50a-3d1b473e82a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You told me your name is Bob!\n"
     ]
    }
   ],
   "source": [
    "chain = model | parser\n",
    "\n",
    "response = chain.invoke([HumanMessage(content=\"hi I am Bob\"),HumanMessage(content=\"What is my name?\")])\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1910840a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice to meet you, Bob!\n"
     ]
    }
   ],
   "source": [
    "chain = model | parser\n",
    "\n",
    "response = chain.invoke([HumanMessage(content=\"What is my name?\"),HumanMessage(content=\"hi I am Bob\")])\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582ff0e7",
   "metadata": {},
   "source": [
    "#### Lets dig into what is happening here.\n",
    "1. Click here to check the UML diagram: \n",
    "2. https://medium.com/azure-monitor-from-a-programmers-perspective/langchain-ii-basic-chatbot-unpacked-a60510b9ac6b#56cf\n",
    "\n",
    "\n",
    "#### Runnable\n",
    "1. Its an extremely prominent class and used extensively in creating chains.\n",
    "2. Chains combine components together in a pipeline\n",
    "3. Many components like all models, parsers, prompts and anything that can logically go into a chain derives from it.\n",
    "4. `ChatGroq` is provided partner by extends `BaseChatModel` from langchain_core\n",
    "5. https://github.com/langchain-ai/langchain/blob/master/libs/partners/groq/langchain_groq/chat_models.py\n",
    "6. This is the base class for all model classes offered by any partner.\n",
    "7. `BaseClass` extends `RunnableSerializable` that supports serialization into JSON\n",
    "8. `RunnableSerializable` extends `Runnable` that means it can participate in chains.\n",
    "9. You can also use `RunnableSequence` to construct the chain.\n",
    "10. https://github.com/langchain-ai/langchain/blob/master/libs/core/langchain_core/runnables/base.py#L2659"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85ecfeb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Bob! It's nice to meet you. Is there something I can help you with or would you like to chat?\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableSequence\n",
    "chain = RunnableSequence(model, parser)\n",
    "chain.invoke([HumanMessage(content=\"hi i am bob\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d32d6abc",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m      3\u001b[0m     chain\u001b[38;5;241m.\u001b[39minvoke(msg_hist)\n\u001b[0;32m----> 4\u001b[0m     user_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEnter your query?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     msg_hist\u001b[38;5;241m.\u001b[39mappend(user_input)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/ipykernel/kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/ipykernel/kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "msg_hist=[SystemMessage(content='your task is to answer query to user')]\n",
    "while True:\n",
    "    chain.invoke(msg_hist)\n",
    "    user_input=input(\"Enter your query?\")\n",
    "    msg_hist.append(user_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7c363b",
   "metadata": {},
   "source": [
    "1. Chain calls the first component and passes any arguments provided to it.\n",
    "2. In this case its an object of type `HumanMessage`\n",
    "3. This is how a chain looks: https://miro.medium.com/v2/resize:fit:750/format:webp/1*K1F-m4gImEUO0AELkpQuKg.jpeg\n",
    "4. Each model component by any partner provides an object of type `BaseMessage`. This is then passed to the next component.\n",
    "5. This is the signature of invoke of a model class\n",
    "\n",
    "`def` `invoke(str | List[dict | tuple | BaseMessage] | PromptValue):`\\\n",
    "    Suite\n",
    "  \n",
    "6. In our example `HumanMessage` is derived from `BaseMessage` which needs `content` for initialization.\n",
    "\n",
    "`param content: Union[str, List[Union[str,Dict]]]`\n",
    "\n",
    "7. Union, List, Dict are all defined in typing module\n",
    "8. Union means one of the input types is expected. We are passing a string.\n",
    "\n",
    "9. Our `parser` is of type `StrOutputParser` that extends `BaseOutputParser`\n",
    "10. Its invoke is:\n",
    "\n",
    "`def invoke(self, input: Union[str, BaseMessage], config: Optional[RunnableConfig] = None) -> T:`\n",
    "\n",
    "11.  This says input can be either string or `BaseMessage`. We are using `BaseMessage` the return type of `model`\n",
    "\n",
    "12. Some useful methods are:\n",
    "- parser.input_schema.schema() # get JSON schema of the input\n",
    "- parser.output_schema.schema() # gets JSON schema of the output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807a9b92",
   "metadata": {},
   "source": [
    "### Adding history to chat\n",
    "1. At this stage if you pass another message to the model it will have no recollection of the earlier message.\n",
    "2. Lets add history. Chat history is managed by a set of classes offered by community.\n",
    "3. https://github.com/langchain-ai/langchain/blob/master/libs/core/langchain_core/chat_history.py\n",
    "4. `asyncio` is a Python library: https://docs.python.org/3/library/asyncio.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c94cf4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Messages retrieved from DB\n",
      "Hi Bob! It's nice to meet you. Is there something I can help you with or would you like to chat?\n",
      "Messages retrieved from DB\n",
      "[HumanMessage(content='Hi! I am Bob', additional_kwargs={}, response_metadata={}), SystemMessage(content=\"Hi Bob! It's nice to meet you. Is there something I can help you with or would you like to chat?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Lets see if you know my name dude?', additional_kwargs={}, response_metadata={})]\n",
      "You introduced yourself as Bob, nice to meet you, Bob! How's your day going so far?\n"
     ]
    }
   ],
   "source": [
    "# import the chat history classes\n",
    "from langchain_core.chat_history import (\n",
    "    BaseChatMessageHistory,\n",
    "    InMemoryChatMessageHistory,\n",
    ")\n",
    "import asyncio # library for writing code that interacts with DB, network calls etc. \n",
    "\n",
    "#Create a store in memory\n",
    "store = InMemoryChatMessageHistory()\n",
    "\n",
    "\n",
    "# Lets define a function that gets messages from store\n",
    "async def getMessage():\n",
    "    await asyncio.sleep(2) # this will mimic a read from DB\n",
    "    print(\"Messages retrieved from DB\")\n",
    "    return await store.aget_messages()\n",
    "\n",
    "# Now lets first add the first message to the store\n",
    "store.add_message(HumanMessage('Hi! I am Bob'))\n",
    "\n",
    "messages = await(getMessage())\n",
    "\n",
    "\n",
    "response = model.invoke(messages) # asyncio has runners for coroutines, context managers etc. \n",
    "print(response.content) # note that our first message is safely in the store\n",
    "\n",
    "# lets add the message returned by the model to the store\n",
    "\n",
    "store.add_message(SystemMessage(response.content))\n",
    "\n",
    "\n",
    "store.add_message(HumanMessage('Lets see if you know my name dude?'))\n",
    "\n",
    "messages = await(getMessage())\n",
    "\n",
    "print(messages) # check all the message are in store.\n",
    "\n",
    "response = model.invoke(messages)\n",
    "\n",
    "print(response.content) # Notice that the reponse now takes into account earlier interactions also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8851c564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Messages retrieved from DB\n",
      "In Python, a list is a collection of items that can be of any data type, including strings, integers, floats, and other lists.\n",
      "Messages retrieved from DB\n",
      "[HumanMessage(content='Hi! I am Bob', additional_kwargs={}, response_metadata={}), SystemMessage(content=\"Hi Bob! It's nice to meet you. Is there something I can help you with or would you like to chat?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Lets see if you know my name dude?', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hi! what are lists in python?', additional_kwargs={}, response_metadata={}), SystemMessage(content=\"A nice change of subject!\\n\\nIn Python, a list is a data structure that can store multiple values in a single variable. It's a collection of items that can be of any data type, including strings, integers, floats, and other lists.\\n\\nLists are denoted by square brackets `[]` and are typically used to store a sequence of values. You can think of a list as a container that can hold multiple items, and you can access each item by its index (position in the list).\\n\\nHere's an example of creating a list in Python:\\n```\\nfruits = ['apple', 'banana', 'cherry']\\n```\\nIn this example, `fruits` is a list that contains three strings: `'apple'`, `'banana'`, and `'cherry'`.\\n\\nYou can access individual items in the list by their index, starting from 0. For example:\\n```\\nprint(fruits[0])  # Output: 'apple'\\nprint(fruits[1])  # Output: 'banana'\\nprint(fruits[2])  # Output: 'cherry'\\n```\\nYou can also modify lists by adding, removing, or modifying items. For example:\\n```\\nfruits.append('orange')  # Add a new item to the end of the list\\nprint(fruits)  # Output: ['apple', 'banana', 'cherry', 'orange']\\n\\nfruits.remove('banana')  # Remove the item at index 1\\nprint(fruits)  # Output: ['apple', 'cherry', 'orange']\\n```\\nLists are a fundamental data structure in Python and are used extensively in programming. They provide a flexible way to store and manipulate data, making them a powerful tool in your Python toolkit!\", additional_kwargs={}, response_metadata={}), HumanMessage(content='how does it differ from strings?', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hi! what are lists in python?', additional_kwargs={}, response_metadata={}), SystemMessage(content=\"In Python, a list is a data structure that can store multiple values in a single variable. It's a collection of items that can be of any data type, including strings, integers, floats, and other lists.\\n\\nLists are denoted by square brackets `[]` and are typically used to store a sequence of values. You can think of a list as a container that can hold multiple items, and you can access each item by its index (position in the list).\\n\\nHere's an example of creating a list in Python:\\n```\\nfruits = ['apple', 'banana', 'cherry']\\n```\\nIn this example, `fruits` is a list that contains three strings: `'apple'`, `'banana'`, and `'cherry'`.\\n\\nOne key difference between lists and strings is that lists are mutable, meaning you can modify them after they're created. You can add, remove, or modify items in a list. Strings, on the other hand, are immutable, meaning you can't change their contents once they're created.\\n\\nFor example, you can modify the `fruits` list like this:\\n```\\nfruits.append('orange')  # Add a new item to the end of the list\\nfruits[1] = 'pineapple'  # Replace the second item with a new value\\n```\\nBut you can't modify a string in the same way:\\n```\\nmy_string = 'hello'\\nmy_string[0] = 'H'  # This will raise a TypeError\\n```\\nI hope that helps! Let me know if you have any other questions.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='how does it differ from strings?', additional_kwargs={}, response_metadata={}), SystemMessage(content=\"In Python, a list is a collection of items that can be of any data type, including strings, integers, floats, and other lists. Lists are denoted by square brackets `[]` and are typically used to store a sequence of values. You can think of a list as a container that can hold multiple items, and you can access each item by its index (position in the list).\\n\\nOne key difference between lists and strings is that lists are mutable, meaning you can modify them after they're created. You can add, remove, or modify items in a list. Strings, on the other hand, are immutable, meaning you can't change their contents once they're created.\\n\\nFor example, you can modify the `fruits` list like this:\\n```\\nfruits = ['apple', 'banana', 'cherry']\\nfruits.append('orange')  # Add a new item to the end of the list\\nfruits[1] = 'pineapple'  # Replace the second item with a new value\\n```\\nBut you can't modify a string in the same way:\\n```\\nmy_string = 'hello'\\nmy_string[0] = 'H'  # This will raise a TypeError\\n```\\nThis is because strings are immutable, so once they're created, you can't change their contents. Lists, on the other hand, are mutable, so you can modify them as needed.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='list vs strings in breif', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hi! what are lists in python?', additional_kwargs={}, response_metadata={}), SystemMessage(content='Hi Bob!', additional_kwargs={}, response_metadata={}), HumanMessage(content='how does it differ from strings?', additional_kwargs={}, response_metadata={}), SystemMessage(content=\"In Python, a list is a collection of items that can be of any data type, including strings, integers, floats, and other lists. Lists are denoted by square brackets `[]` and are typically used to store a sequence of values.\\n\\nOne key difference between lists and strings is that lists are mutable, meaning you can modify them after they're created. You can add, remove, or modify items in a list. Strings, on the other hand, are immutable, meaning you can't change their contents once they're created.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='list vs strings in breif', additional_kwargs={}, response_metadata={}), SystemMessage(content='Lists:\\n\\n* Can store multiple values in a single variable\\n* Items can be of any data type, including strings, integers, floats, and other lists\\n* Are mutable, meaning you can add, remove, or modify items after creation\\n* Are denoted by square brackets `[]`\\n\\nStrings:\\n\\n* Are a sequence of characters\\n* Are immutable, meaning you can\\'t change their contents once they\\'re created\\n* Can be denoted by single quotes `\\'` or double quotes `\"`\\n\\nIn short, lists are like containers that can hold multiple values and can be modified, while strings are a single sequence of characters that can\\'t be changed once created.', additional_kwargs={}, response_metadata={}), HumanMessage(content='what is my name?', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hi! what are lists in python?', additional_kwargs={}, response_metadata={}), SystemMessage(content='In Python, a list is a collection of items that can be of any data type, including strings, integers, floats, and other lists.', additional_kwargs={}, response_metadata={}), HumanMessage(content='list vs strings in breif', additional_kwargs={}, response_metadata={})]\n",
      "In Python, a list is a collection of items that can be of any data type, including strings, integers, floats, and other lists. Lists are denoted by square brackets `[]`.\n"
     ]
    }
   ],
   "source": [
    "store.add_message(HumanMessage('Hi! what are lists in python?'))\n",
    "messages = await(getMessage())\n",
    "response = model.invoke(messages)\n",
    "print(response.content)\n",
    "\n",
    "store.add_message(SystemMessage(response.content))\n",
    "store.add_message(HumanMessage('list vs strings in breif'))\n",
    "messages = await(getMessage())\n",
    "print(messages)\n",
    "response = model.invoke(messages)\n",
    "print(response.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e221d9",
   "metadata": {},
   "source": [
    "1. There are some issues here. Since Chat History is not a descendant of Runnable we cannot chain it.\n",
    "2. Therefore the code is sort of littered. \n",
    "3. Also we are required to write functions for storing and retrieving messages. This should be rather standard and done by the framework!\n",
    "4. What about sessions? This code is running of the server which supports multiple users. So there needs to be a mechanism to manage sessions.\n",
    "\n",
    "#### RunnableWithMessageHistory\n",
    "1. This is where LangChain offers this class.\n",
    "2. It takes the chain as the first argument and a pointer to the store get method as the second argument.\n",
    "3. This class then takes the ownership of executing the chain and any component that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "949191e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BaseChatMessageHistory' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrunnables\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhistory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RunnableWithMessageHistory\n\u001b[1;32m      5\u001b[0m store \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_session_history\u001b[39m(session_id: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[43mBaseChatMessageHistory\u001b[49m:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m session_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m store:  \u001b[38;5;66;03m# If a new session then create a new memory store.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m         store[session_id] \u001b[38;5;241m=\u001b[39m InMemoryChatMessageHistory()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BaseChatMessageHistory' is not defined"
     ]
    }
   ],
   "source": [
    "# Lets create our own store. This store will be a dict with a key for each session\n",
    "# The value for each key will be InMemoryChatHistory object \n",
    "\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:  # If a new session then create a new memory store.\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "config = {'configurable': {\"session_id\": \"abc2\"}}\n",
    "withHistory = RunnableWithMessageHistory(model, get_session_history)\n",
    "\n",
    "response = withHistory.invoke([HumanMessage(content=\"Hi! I am Bob\")], config=config)\n",
    "\n",
    "print(response.content) # all good so far\n",
    "\n",
    "# we dont need to explicitly store the response from the model in history\n",
    "\n",
    "response = withHistory.invoke(\n",
    "    [HumanMessage(content=\"Lets see if you know my name dude?\")], config=config\n",
    ")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81909f45",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RunnableWithMessageHistory' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m withHistory1 \u001b[38;5;241m=\u001b[39m \u001b[43mRunnableWithMessageHistory\u001b[49m(model, get_session_history)\n\u001b[1;32m      2\u001b[0m response \u001b[38;5;241m=\u001b[39m withHistory\u001b[38;5;241m.\u001b[39minvoke([HumanMessage(content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHi! I am Priyanka\u001b[39m\u001b[38;5;124m\"\u001b[39m)], config\u001b[38;5;241m=\u001b[39mconfig)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mcontent)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RunnableWithMessageHistory' is not defined"
     ]
    }
   ],
   "source": [
    "withHistory1 = RunnableWithMessageHistory(model, get_session_history)\n",
    "response = withHistory.invoke([HumanMessage(content=\"Hi! I am Priyanka\")], config=config)\n",
    "print(response.content)\n",
    "response = withHistory.invoke(\n",
    "    [HumanMessage(content=\"Lets see if you know my name dude?\")], config=config\n",
    ")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c080dd6",
   "metadata": {},
   "source": [
    "1. Here is a flowchart of this program.\n",
    "2. https://medium.com/azure-monitor-from-a-programmers-perspective/langchain-ii-basic-chatbot-unpacked-a60510b9ac6b#3c92\n",
    "3. Wrapper around another runnable - the chain\n",
    "4. https://techblogs.cloudlex.com/langchain-ii-basic-chatbot-unpacked-a60510b9ac6b#a0cb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6d9303",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
